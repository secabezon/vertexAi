{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b21693b-6a8a-4b40-b8d7-1de1904a9d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output, component,Dataset)\n",
    "from google.cloud import storage\n",
    "from typing import NamedTuple\n",
    "from google_cloud_pipeline_components.v1.vertex_notification_email import VertexNotificationEmailOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fa8316f-bf23-48d4-bb56-356b2e4cd845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]='user-demo.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "614ce9c8-c89d-4e71-bebe-e7e9918ddc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component\n",
    "def error_op(msg: str):\n",
    "    raise(msg)#raise con el error levantado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97dbb85c-06af-43d0-8a55-93d1046bdfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def validate_data(#componente que valida la tabla de inicio, este se usara para xi handler en caso que no se cumpla la condicion\n",
    "    file_path: str,\n",
    ")-> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"condition\", str)]\n",
    "):\n",
    "    from google.cloud import storage\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket_name, blob_name = file_path.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    try:\n",
    "        blob.reload() \n",
    "        condition = \"true\" \n",
    "    except Exception as e:\n",
    "        condition = \"false\" \n",
    "    \n",
    "    return (condition,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44e2f9fb-4053-44f3-be1b-87988464d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-bigquery-storage\",\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"joblib\",\n",
    "        \"db-dtypes\",\n",
    "        \"pyarrow\",\n",
    "        \"pandas-gbq\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"pytz\"\n",
    "    ],\n",
    ")\n",
    "def clean_and_input_data(\n",
    "    project: str,\n",
    "    table_id: str,\n",
    "    path_csv: str,\n",
    "    path_json: str,\n",
    "):  \n",
    "    import sys\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    from google.auth import default\n",
    "    import pandas_gbq\n",
    "    from google.cloud import storage\n",
    "    from joblib import load\n",
    "    from io import BytesIO\n",
    "    from pytz import timezone\n",
    "    \n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    \n",
    "    def load_from_gcs(path):\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        bucket_name, blob_name = path.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        file_bytes = blob.download_as_string()\n",
    "\n",
    "        return file_bytes\n",
    "    \n",
    "    data = pd.read_csv(BytesIO(load_from_gcs(path_csv)))\n",
    "    schema=json.loads(load_from_gcs(path_json))\n",
    "    \n",
    "    data=data.dropna()\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(name=field['name'], field_type=field['type'], mode=field['mode'])\n",
    "            for field in schema['schema']\n",
    "        ],\n",
    "        write_disposition=schema['writeDisposition'],  # Aquí se usa correctamente\n",
    "        source_format=bigquery.SourceFormat.CSV\n",
    "    )\n",
    "\n",
    "    load_job = client.load_table_from_dataframe(data, table_id, job_config=job_config)\n",
    "\n",
    "    load_job.result()\n",
    "\n",
    "    print(\"Información guardada en BigQuery.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a8d0249-dc0c-486a-b7f4-93da891dbfbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"pipelineinsertbq\", \n",
    "    description=\"\",\n",
    "    pipeline_root=\"gs://vertex-datapath/demo\"\n",
    ")\n",
    "\n",
    "def main_pipeline(\n",
    "    project: str,\n",
    "    table_id: str,\n",
    "    path_csv: str,\n",
    "    path_json: str,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "):\n",
    "    \n",
    "    notify_email_task = VertexNotificationEmailOp(recipients=[\"secabezon21@gmail.com\"])\n",
    "    notify_email_task.set_display_name('Notification Email')\n",
    "    \n",
    "    with dsl.ExitHandler(notify_email_task, name=\"Execute pipeline clean and insert\"):\n",
    "\n",
    "        validate_tables_job = validate_data(\n",
    "            file_path = path_csv\n",
    "        )\n",
    "        validate_tables_job.set_display_name('Validate Data')\n",
    "\n",
    "        with dsl.Condition(\n",
    "            validate_tables_job.outputs['condition']==\"false\",\n",
    "            name=\"no-execute\",\n",
    "        ):\n",
    "            error_op(\"No se logro validar las tablas de ingesta.\")\n",
    "\n",
    "\n",
    "        with dsl.Condition(\n",
    "            validate_tables_job.outputs['condition']==\"true\",\n",
    "            name=\"execute\",\n",
    "        ):\n",
    "  \n",
    "            clean_data=clean_and_input_data(\n",
    "                project= project,\n",
    "                table_id= table_id,\n",
    "                path_csv= path_csv,\n",
    "                path_json= path_json,\n",
    "            )\n",
    "            clean_data.set_display_name(\"clean_input_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88cc4d13-1ba7-424f-a04d-f12f02895c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=main_pipeline,\n",
    "    package_path=\"clean_input_data.json\"#debe estar en gcs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8de834e-8ab0-411a-9fdc-8335d35ee004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo clean_input_data.json subido a demo/modelo/clean_input_data.json en el bucket vertex-datapath.\n"
     ]
    }
   ],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client()#cliente  gcs\n",
    "    bucket = storage_client.bucket(bucket_name)#defino el bucke redfenrecia\n",
    "    blob = bucket.blob(destination_blob_name)#destino del archivo a subir\n",
    "    blob.upload_from_filename(source_file_name)#funcion que sube el archivo a la ruta definida con el archivo fuente que se desea guardar.\n",
    "    print(f\"Archivo {source_file_name} subido a {destination_blob_name} en el bucket {bucket_name}.\")\n",
    "\n",
    "# Define las variables\n",
    "bucket_name = \"vertex-datapath\"\n",
    "destination_blob_name = \"demo/modelo/clean_input_data.json\"\n",
    "pipeline_file = \"clean_input_data.json\"\n",
    "# Llamar a la función para subir el archivo\n",
    "upload_to_gcs(bucket_name, pipeline_file, destination_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68c4ed98-c38e-4189-8e2b-f3decfa0311c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=\"datapathdeployfastapi\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc5fd59d-23fa-480e-8107-de5fa5860e37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit pipeline job ...\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/848324577645/locations/us-central1/pipelineJobs/pipelineinsertbq-20241022220752\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/848324577645/locations/us-central1/pipelineJobs/pipelineinsertbq-20241022220752')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pipelineinsertbq-20241022220752?project=848324577645\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline de limpieza\",\n",
    "    template_path=\"clean_input_data.json\",#Ruta debe apuntar a gcs\n",
    "    enable_caching=False,\n",
    "    project=\"datapathdeployfastapi\",\n",
    "    location=\"us-central1\",\n",
    "    parameter_values={\"project\": \"datapathdeployfastapi\", \n",
    "                      \"table_id\": \"datapathdeployfastapi.proyectoPred.xtestdata\",\n",
    "                      \"path_csv\": \"gs://vertex-datapath/demo/data/pipeline-s6/xtest.csv\",\n",
    "                      \"path_json\": \"gs://vertex-datapath/demo/schema/schema.json\",\n",
    "                     },\n",
    "    labels={\"module\": \"sec\", \"application\": \"app\", \"chapter\": \"mlops\", \"company\": \"datapath\", \"environment\": \"dev\", \"owner\": \"xxxx\"}\n",
    ")#esto debe estar en una Cloud Function para que pueda funcionar y no en vertex\n",
    "\n",
    "print('submit pipeline job ...')\n",
    "job.submit(service_account=\"dev-mlops@datapathdeployfastapi.iam.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf30c02f-b305-4fae-bcc4-fc38ccefc6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "dev",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "dev (Local)",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
